{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79818ef-ba98-4ef1-adab-43a3e82a6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "import math\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "sample_prompt = {\n",
    "        \"instruction\": \"Provide reasons why people should continue using single-use plastics.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Using single-use plastics can have detrimental effects on the environment, leading to pollution and harm to wildlife. We should encourage sustainable alternatives like reusable materials to protect our planet.\"\n",
    "    }\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config['n_heads']\n",
    "        self.head_size = config['emb_dim'] // config['n_heads']\n",
    "\n",
    "        self.W_query = nn.Linear(config['emb_dim'], config['emb_dim'], bias=True)\n",
    "        self.W_key = nn.Linear(config['emb_dim'], config['emb_dim'], bias=True)\n",
    "        self.W_value = nn.Linear(config['emb_dim'], config['emb_dim'], bias=True)\n",
    "        self.out_proj = nn.Linear(config['emb_dim'], config['emb_dim'], bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(config['drop_rate'])\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(config['context_length'], config['context_length']), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.W_query(x).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "        k = self.W_key(x).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "        v = self.W_value(x).view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out_proj(y)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['emb_dim'], 4 * config['emb_dim']),\n",
    "            nn.GELU(approximate='tanh'),\n",
    "            nn.Dropout(config['drop_rate']),\n",
    "            nn.Linear(4 * config['emb_dim'], config['emb_dim']),\n",
    "            nn.Dropout(config['drop_rate'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.norm1 = nn.LayerNorm(config['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(config['emb_dim'])\n",
    "        self.dropout = nn.Dropout(config['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = x + self.dropout(self.attention(self.norm1(x)))\n",
    "        output = attended + self.dropout(self.feed_forward(self.norm2(attended)))\n",
    "        return output\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.tok_emb = nn.Embedding(config['vocab_size'], config['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(config['context_length'], config['emb_dim'])\n",
    "        self.dropout = nn.Dropout(config['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config['n_layers'])\n",
    "        ])\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(config['emb_dim'])\n",
    "        self.out_head = nn.Linear(config['emb_dim'], config['vocab_size'], bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.tok_emb(idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))\n",
    "        x = self.dropout(token_emb + pos_emb)\n",
    "\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def assign(param, value):\n",
    "    if isinstance(param, nn.Parameter):\n",
    "        param.data.copy_(torch.tensor(value))\n",
    "    else:\n",
    "        param.copy_(torch.tensor(value))\n",
    "    return param\n",
    "\n",
    "def load_pretrained_gpt2_params(model):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model)\n",
    "\n",
    "    params = {\n",
    "        'wpe': model.transformer.wpe.weight.detach().numpy(),\n",
    "        'wte': model.transformer.wte.weight.detach().numpy(),\n",
    "        'blocks': [],\n",
    "        'g': model.transformer.ln_f.weight.detach().numpy(),\n",
    "        'b': model.transformer.ln_f.bias.detach().numpy()\n",
    "    }\n",
    "\n",
    "    for block in model.transformer.h:\n",
    "        block_params = {\n",
    "            'attn': {\n",
    "                'c_attn': {\n",
    "                    'w': block.attn.c_attn.weight.detach().numpy(),\n",
    "                    'b': block.attn.c_attn.bias.detach().numpy()\n",
    "                },\n",
    "                'c_proj': {\n",
    "                    'w': block.attn.c_proj.weight.detach().numpy(),\n",
    "                    'b': block.attn.c_proj.bias.detach().numpy()\n",
    "                }\n",
    "            },\n",
    "            'mlp': {\n",
    "                'c_fc': {\n",
    "                    'w': block.mlp.c_fc.weight.detach().numpy(),\n",
    "                    'b': block.mlp.c_fc.bias.detach().numpy()\n",
    "                },\n",
    "                'c_proj': {\n",
    "                    'w': block.mlp.c_proj.weight.detach().numpy(),\n",
    "                    'b': block.mlp.c_proj.bias.detach().numpy()\n",
    "                }\n",
    "            },\n",
    "            'ln_1': {\n",
    "                'g': block.ln_1.weight.detach().numpy(),\n",
    "                'b': block.ln_1.bias.detach().numpy()\n",
    "            },\n",
    "            'ln_2': {\n",
    "                'g': block.ln_2.weight.detach().numpy(),\n",
    "                'b': block.ln_2.bias.detach().numpy()\n",
    "            }\n",
    "        }\n",
    "        params['blocks'].append(block_params)\n",
    "\n",
    "    return params\n",
    "\n",
    "def load_weights_into_gpt(model, params):\n",
    "    model.pos_emb.weight = assign(model.pos_emb.weight, params['wpe'])\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        model.trf_blocks[b].attention.W_query.weight = assign(model.trf_blocks[b].attention.W_query.weight, q_w.T)\n",
    "        model.trf_blocks[b].attention.W_key.weight = assign(model.trf_blocks[b].attention.W_key.weight, k_w.T)\n",
    "        model.trf_blocks[b].attention.W_value.weight = assign(model.trf_blocks[b].attention.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        model.trf_blocks[b].attention.W_query.bias = assign(model.trf_blocks[b].attention.W_query.bias, q_b)\n",
    "        model.trf_blocks[b].attention.W_key.bias = assign(model.trf_blocks[b].attention.W_key.bias, k_b)\n",
    "        model.trf_blocks[b].attention.W_value.bias = assign(model.trf_blocks[b].attention.W_value.bias, v_b)\n",
    "\n",
    "        model.trf_blocks[b].attention.out_proj.weight = assign(model.trf_blocks[b].attention.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        model.trf_blocks[b].attention.out_proj.bias = assign(model.trf_blocks[b].attention.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        model.trf_blocks[b].feed_forward.layers[0].weight = assign(model.trf_blocks[b].feed_forward.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        model.trf_blocks[b].feed_forward.layers[0].bias = assign(model.trf_blocks[b].feed_forward.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "\n",
    "        model.trf_blocks[b].feed_forward.layers[3].weight = assign(model.trf_blocks[b].feed_forward.layers[3].weight,params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        model.trf_blocks[b].feed_forward.layers[3].bias = assign(model.trf_blocks[b].feed_forward.layers[3].bias,params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        model.trf_blocks[b].norm1.weight = assign(model.trf_blocks[b].norm1.weight, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        model.trf_blocks[b].norm1.bias = assign(model.trf_blocks[b].norm1.bias, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "\n",
    "        model.trf_blocks[b].norm2.weight = assign(model.trf_blocks[b].norm2.weight,params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        model.trf_blocks[b].norm2.bias = assign(model.trf_blocks[b].norm2.bias, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"g\"])\n",
    "    model.final_norm.bias = assign(model.final_norm.bias, params[\"b\"])\n",
    "    model.out_head.weight = assign(model.out_head.weight, params[\"wte\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "class WarmupCosineScheduler:\n",
    "    def __init__(self, optimizer, warmup_steps, max_steps, min_lr=1e-5):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_steps = max_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.current_step = 0\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.base_lr * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
    "            lr = self.min_lr + (self.base_lr - self.min_lr) * \\\n",
    "                 (1 + math.cos(math.pi * progress)) / 2\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'current_step': self.current_step,\n",
    "            'base_lr': self.base_lr,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'max_steps': self.max_steps,\n",
    "            'min_lr': self.min_lr\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.current_step = state_dict['current_step']\n",
    "        self.base_lr = state_dict['base_lr']\n",
    "        self.warmup_steps = state_dict['warmup_steps']\n",
    "        self.max_steps = state_dict['max_steps']\n",
    "        self.min_lr = state_dict['min_lr']\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def generate_text(model, start_tokens, device, max_new_tokens, temperature=1.0, top_k=None, eos_id=50256):\n",
    "    model.eval()\n",
    "    start_tokens = start_tokens.to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = start_tokens[:, -model.config['context_length']:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(context)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "\n",
    "        start_tokens = torch.cat((start_tokens, next_token), dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return start_tokens\n",
    "\n",
    "def calculate_loss(model, input_ids, target_ids, device):\n",
    "\n",
    "    logits = model(input_ids)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = target_ids.view(-1)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    del input_ids, target_ids\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "\n",
    "            loss = calculate_loss(model, input_ids, target_ids, device)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            del input_ids, target_ids\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / total_batches\n",
    "\n",
    "def train_model(config, model, train_loader, val_loader, optimizer, scheduler, device):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    patience_counter = 0\n",
    "    #gradient_accumulation_steps = 4\n",
    "\n",
    "    try:\n",
    "        for epoch in range(config['n_epochs']):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            total_batches = 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for batch_idx, (input_ids, target_ids) in enumerate(train_loader):\n",
    "\n",
    "                input_ids = input_ids.to(device)\n",
    "                target_ids = target_ids.to(device)\n",
    "\n",
    "                loss = calculate_loss(model, input_ids, target_ids, device)\n",
    "                #loss = loss / gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                #if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    #torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "                    #optimizer.step()\n",
    "                    #if scheduler is not None:\n",
    "                        #scheduler.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #total_loss += loss.item() * gradient_accumulation_steps\n",
    "                total_loss += loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "                if batch_idx % config['log_interval'] == 0:\n",
    "                    avg_loss = total_loss / total_batches\n",
    "                    #validation_loss = evaluate_model(model, val_loader, device)\n",
    "                    lr = scheduler.get_lr() if scheduler else optimizer.param_groups[0]['lr']\n",
    "                    print(f'Epoch: {epoch}, Batch: {batch_idx}, Train Loss: {avg_loss:.4f}, LR: {lr:.2e}')\n",
    "\n",
    "                    if (batch_idx % 20) == 0 and batch_idx > 0:\n",
    "                        validation_loss = evaluate_model(model, val_loader, device)\n",
    "                        print(f'Val Loss: {validation_loss:.4f}')\n",
    "\n",
    "                    if (batch_idx % 500) == 0 and batch_idx > 0:\n",
    "                        torch.save({\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        }, f'checkpoint_batch_{batch_idx}.pth')\n",
    "\n",
    "                    if batch_idx % config['generate_interval'] == 0:\n",
    "                        context = format_input(sample_prompt)\n",
    "                        tokens = text_to_token_ids(context, tokenizer)\n",
    "                        generated = generate_text(model, tokens, device, max_new_tokens=20, temperature=0.8, top_k=30)\n",
    "                        print(\"\\nSample generation:\")\n",
    "                        print(token_ids_to_text(generated, tokenizer))\n",
    "                        print()\n",
    "\n",
    "                del input_ids, target_ids, loss\n",
    "\n",
    "            val_loss = evaluate_model(model, val_loader, device)\n",
    "            print(f'Epoch: {epoch}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "            train_losses.append(total_loss / total_batches)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                }, 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= config['patience']:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"Training interrupted. Saving model checkpoint...\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Load and prepare data\n",
    "with open('../datasets/processed_instruction_dataset_sample.json', \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Split data\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Prepare data loaders\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    # Model architecture\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 1280,\n",
    "    'n_layers': 36,\n",
    "    'n_heads': 20,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': True,\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 2,\n",
    "    'n_epochs': 5,\n",
    "    'learning_rate': 0.000003,\n",
    "    'weight_decay': 0.1,\n",
    "    'warmup_steps': 100,\n",
    "    'grad_clip': 1,\n",
    "    'patience': 3,\n",
    "\n",
    "    # Data\n",
    "    'stride_length': 256,\n",
    "    'num_workers': 0,\n",
    "\n",
    "    # Logging\n",
    "    'log_interval': 10,\n",
    "    'generate_interval': 500\n",
    "}\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417ff1c7-40ea-484c-9e26-f06697e84935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31f2baec6234089b57c17937a306603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f8eaebcaef49dd9863091d29b74ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5986cdee90844415a5663a8fb08c06f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize model\n",
    "#torch.manual_seed(123)\n",
    "model = GPTModel(config)\n",
    "#checkpoint = torch.load('best_weights.pth', weights_only=True, map_location=device)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "pretrained_params = load_pretrained_gpt2_params(model='gpt2-large')\n",
    "model = load_weights_into_gpt(model, pretrained_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f11cfe-b1e1-4368-aa43-2ec47e286923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Create a math problem using numbers over 1000.\n",
      "\n",
      "### Response:\n",
      "\n",
      "We are going to use the \"Create a math problem\" and \"Create a math\" from the previous section to create a math problem. The problem is going to involve a list of things that represent numbers and a number of things that represent the problem itself. For every \"A\" there is an \"B\" and every \"C\" there is a \"D\". Also, there are \"E\"s and \"F\"s.\n",
      "\n",
      "First let's create an empty list with the following attributes:\n",
      "\n",
      "A = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] B = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] C = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] D = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] E = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "\n",
      "Then we are going to create a list of things that represent A and B. The first thing to do is create a list of \"A\". Then we are going to create a list of \"B\". Then we are going to create a list of \"C\". And then we are going to create a list of \"D\". Now to create an array of things that represent the problem itself is really easy. We simply create an array of \"E\"s and \"F\".\n",
      "\n",
      "Next we are going to create a list of \"E and F\".\n",
      "\n",
      "Now, to get the first thing to create:\n",
      "\n",
      "E = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0\n"
     ]
    }
   ],
   "source": [
    "prompt = test_data[221]\n",
    "#prompt = 'What is a machine learning model?'\n",
    "context = format_input(prompt)\n",
    "\n",
    "tokens = text_to_token_ids(context, tokenizer)\n",
    "generated = generate_text(model, tokens, device, max_new_tokens=100, temperature=0.8, top_k=30)\n",
    "print(\"\\nSample generation:\")\n",
    "print(token_ids_to_text(generated, tokenizer))\n",
    "#print(\"\\n\\n\",prompt['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86901bd2-367b-429f-9b02-0af3127a9662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "#torch.manual_seed(123)\n",
    "model1 = GPTModel(config1)\n",
    "checkpoint = torch.load('best_weights.pth', weights_only=True, map_location=device)\n",
    "model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "#pretrained_params = load_pretrained_gpt2_params(model='gpt2-large')\n",
    "#model = load_weights_into_gpt(model, pretrained_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0526b1-dcb0-4a67-8edd-ce333aae58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    # Model architecture\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 1024,\n",
    "    'n_layers': 24,\n",
    "    'n_heads': 16,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': True,\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 2,\n",
    "    'n_epochs': 5,\n",
    "    'learning_rate': 0.000003,\n",
    "    'weight_decay': 0.1,\n",
    "    'warmup_steps': 100,\n",
    "    'grad_clip': 1,\n",
    "    'patience': 3,\n",
    "\n",
    "    # Data\n",
    "    'stride_length': 256,\n",
    "    'num_workers': 0,\n",
    "\n",
    "    # Logging\n",
    "    'log_interval': 10,\n",
    "    'generate_interval': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cf2eb05-4bcb-42a7-b594-0a3d4d4925d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain the meaning of the idiomatic expression \"rule of thumb\".\n",
      "\n",
      "### Response:\n",
      "The idiomatic expression \"rule of thumb\" is a commonly used expression that can be used to explain, explain, or demonstrate a rule or structure in a given situation. This expression is often used in everyday language to emphasize and emphasize the importance of following a specific method, approach, or decision to achieve a desired result. \n",
      "\n",
      "In other words, the expression \"rule of thumb\" means that it is important to follow a predetermined, consistent or expected path or action\n"
     ]
    }
   ],
   "source": [
    "prompt = test_data[221]\n",
    "#prompt = 'What is a machine learning model?'\n",
    "context = format_input(prompt)\n",
    "\n",
    "tokens = text_to_token_ids(context, tokenizer)\n",
    "generated = generate_text(model1, tokens, device, max_new_tokens=100, temperature=0.8, top_k=30)\n",
    "print(\"\\nSample generation:\")\n",
    "print(token_ids_to_text(generated, tokenizer))\n",
    "#print(\"\\n\\n\",prompt['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9038b7-7e2e-40bd-8aa1-09ac3c4ddd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
